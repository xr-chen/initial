{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了能够很好地模拟真实样本的观测误差，我们给模型添加误差自变量$\\epsilon$，它采样自均 值为 0，方差为 0.01 的高斯分布："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "𝑦=1.477x+0.089+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,0.01)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=[]\n",
    "for i in range(100):\n",
    "    x=np.random.uniform(-10,10)\n",
    "    eps=np.random.normal(0,0.1)\n",
    "    y=1.477*x+0.089+eps\n",
    "    data.append([x,y])\n",
    "data=np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义误差函数：\n",
    "$$\n",
    "\\mathcal{L}=\\sum_{i=0}^{n}(wx^{(i)}+b-y^{(i)})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(b,w,points):\n",
    "    totalError=0\n",
    "    for i in range(0,len(points)):\n",
    "        x=points[i,0]\n",
    "        y=points[i,1]\n",
    "        totalError=totalError+(y-(w*x+b))**2\n",
    "    return totalError/float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算偏导数$$\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{w}} \\quad\\frac{\\partial{\\mathcal{L}}}{\\partial{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current,w_current,points,lr):\n",
    "    b_gradient=0\n",
    "    w_gradient=0\n",
    "    M=float(len(points))\n",
    "    for i in range(0,len(points)):\n",
    "        x=points[i,0]\n",
    "        y=points[i,1]\n",
    "        # 误差函数对 b 的导数：grad_b = 2(wx+b-y)，\n",
    "        b_gradient=b_gradient+(2/M)*((w_current*x+b_current)-y)\n",
    "        w_gradient=w_gradient+(2/M)*x*((w_current*x+b_current)-y)\n",
    "    new_b=b_current-(lr*b_gradient)\n",
    "    new_w=w_current-(lr*w_gradient)\n",
    "    return(new_b,new_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算出误差函数在w和b处的梯度后，我们可以更新w和b的取值，我们把对数据集的所有样本训练一次称为一个Epoch，共循环迭代num_iterations个Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(points,starting_b,starting_w,lr,num_iterations):\n",
    "    b=starting_b\n",
    "    w=starting_w\n",
    "    for step in range(num_iterations):\n",
    "        b,w=step_gradient(b,w,np.array(points),lr)\n",
    "        loss=mse(b,w,points)#计算当前的均方误差，监控训练进度\n",
    "        if step%50==0:#打印误差和实时的w,b值\n",
    "            print(f\"iteration:{step},loss:{loss},w:{w},b:{b}\")\n",
    "    return [b,w]#返回最后一次的w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义主训练函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0,loss:5.582514830509888,w:1.0899745412140727,b:0.016216730381409184\n",
      "iteration:50,loss:0.009282539530900975,w:1.4779034358745997,b:0.07295500223058995\n",
      "iteration:100,loss:0.008517190788099734,w:1.4776509317213995,b:0.09184908751576783\n",
      "iteration:150,loss:0.008414355324518511,w:1.4775583745030108,b:0.09877485073918538\n",
      "iteration:200,loss:0.00840053792079317,w:1.4775244469870976,b:0.10131353919659762\n",
      "iteration:250,loss:0.00839868135653936,w:1.477512010611458,b:0.10224411375941353\n",
      "iteration:300,loss:0.008398431900787984,w:1.4775074519682776,b:0.10258522257661858\n",
      "iteration:350,loss:0.00839839838286811,w:1.4775057809647434,b:0.10271025847360557\n",
      "iteration:400,loss:0.008398393879259971,w:1.4775051684463467,b:0.1027560912833918\n",
      "iteration:450,loss:0.008398393274136296,w:1.477504943923311,b:0.10277289163036028\n",
      "iteration:500,loss:0.008398393192829345,w:1.477504861622773,b:0.10277904991870393\n",
      "iteration:550,loss:0.008398393181904612,w:1.4775048314549173,b:0.10278130728370914\n",
      "iteration:600,loss:0.008398393180436722,w:1.4775048203966723,b:0.10278213473713171\n",
      "iteration:650,loss:0.008398393180239468,w:1.4775048163431928,b:0.10278243804615135\n",
      "iteration:700,loss:0.008398393180212974,w:1.4775048148573606,b:0.10278254922625989\n",
      "iteration:750,loss:0.008398393180209424,w:1.477504814312718,b:0.10278258998013043\n",
      "iteration:800,loss:0.008398393180208925,w:1.4775048141130755,b:0.10278260491875563\n",
      "iteration:850,loss:0.008398393180208885,w:1.477504814039895,b:0.10278261039461642\n",
      "iteration:900,loss:0.00839839318020887,w:1.4775048140130702,b:0.10278261240183272\n",
      "iteration:950,loss:0.008398393180208864,w:1.4775048140032374,b:0.1027826131375923\n",
      "Final loss:0.008398393180208881,w:1.4775048139996754,b:0.10278261340412607\n"
     ]
    }
   ],
   "source": [
    "lr=0.01\n",
    "initial_b=0\n",
    "initial_w=0\n",
    "num_iterations=1000\n",
    "[b,w]=gradient_descent(data,initial_b,initial_w,lr,num_iterations)\n",
    "loss=mse(b,w,data)\n",
    "print(f'Final loss:{loss},w:{w},b:{b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和最小二乘法相比，结果极其相近"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47750481, 0.10278261])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.polyfit(data[:,0],data[:,1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用mnist数据集来测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,optimizers,datasets\n",
    "\n",
    "(x,y),(x_val,y_val)=datasets.mnist.load_data('C:\\\\Users\\\\xr chen\\\\Downloads\\\\mnist.npz')\n",
    "x=2*tf.convert_to_tensor(x,dtype=tf.float32)/255-1\n",
    "y=tf.convert_to_tensor(y,dtype=tf.int32)\n",
    "y=tf.one_hot(y,depth=10)\n",
    "print(x.shape,y.shape)\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((x,y))#构建数据集对象\n",
    "train_dataset=train_dataset.batch(512)#批量训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_data()函数返回两个元组(tuple)对象，第一个是训练集，第二个是测试集，每个 tuple 的第一个元素是多个训练图片数据X，第二个元素是训练图片对应的类别数字Y。其中训练集X的大小为(60000,28,28)，代表了 60000 个样本，每个样本由 28 行、28 列构成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 TensorFlow 中加载的 MNIST 数据图片，数值的范围在[0,255]之间。在机器学习中 间，一般希望数据的范围在 0 周围小范围内分布。通过预处理步骤，我们把[0,255]像素范 围归一化(Normalize)到[0,1.]区间，再缩放到[−1,1]区间，从而有利于模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果物体属 于第𝑖类的话，那么索引为𝑖的位置上设置为 1，其他位置设置为 0，我们把这种编码方式叫 做 one-hot 编码(独热编码)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot 编码是非常稀疏 (Sparse)的，相对于数字编码来说，占用较多的存储空间，所以一般在存储时还是采用数字 编码，在计算时，根据需要来把数字编码转换成 One-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(4, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y=tf.constant([0,1,2,3])\n",
    "y=tf.one_hot(y,depth=10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型表达能力偏弱，我们可以给线性模型嵌套一个线性函数。我们把这个非线性函数称为激活函数，用$\\sigma$表示：\n",
    "$$\n",
    "\\bf{o}=\\sigma(\\bf{Wx}+\\bf{b})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的具体的非线性激活函数，比如Sigmoid函数或者ReLU函数，ReLU函数非常简单，仅仅是在y=x的基础上面截去了x<0的部分，仅仅保留正的输入部分，清零负的输入。虽然简单，却有优良的非线性特性，梯度计算简单，这里通过嵌套ReLU函数将模型转化为非线性模型：\n",
    "$$\n",
    "{\\bf{o}}=ReLU({\\bf{Wx}+\\bf{b}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 表达能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对模型的表达能力偏弱的问题，我们可以重复堆叠多次变换增加表达能力\n",
    "$$\n",
    "{\\bf{h_1}}=ReLU({\\bf{W_1x}+\\bf{b_1}})\\\\\n",
    "{\\bf{h_2}}=ReLU({\\bf{W_2h_1}+\\bf{b_2}})\\\\\n",
    "{\\bf{o}}={\\bf{W_3h_2}+\\bf{b_3}}\n",
    "$$\n",
    "把第一层神经元的输出作为第二层神经元的输入，最后一层神经元的输出作为模型的输出${\\bf{o}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于手动计算导数复杂，因此需要借助自动求导技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 对于第一层模型来说，他接受的输入${\\bf{x}}\\in R^{784}$$(28\\cdot28=784)$，输出${\\bf{h_1}} \\in R^{256}$设计为长度为 256 的向量，我们不需要显式地编写${\\bf{𝒉_𝟏}} = ReLU({\\bf{𝑾_𝟏𝒙}}+ {\\bf{𝒃_𝟏}})$的计算逻辑，在 TensorFlow 中通过 一行代码即可实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x20fb87f5988>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Dense(256,activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于3层的网络我们可以通过快速完成3层网络的搭建，第1层输出节点256，第二层输出节点128，输出层输出10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential([\n",
    "    layers.Dense(256,activation='relu'),\n",
    "    layers.Dense(128,activation='relu'),\n",
    "    layers.Dense(10)#和上面的表达式对应，所以第三层网络没有relu，所以有负值的输出\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到模型输出$\\sigma$后，通过MSE损失函数计算当前的误差$\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:#构建梯度记录环境\n",
    "    #打平,[b,28,28]-->[b,784]\n",
    "    x=tf.reshape(x,(-1,28*28))\n",
    "    #Step1.得到模型输出Output\n",
    "    #[b,784]->[b,10]\n",
    "    out=model(x)\n",
    "    loss=tf.reduce_sum(tf.square(out-y))/x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再利用 TensorFlow 提供的自动求导函数tape.gradient(loss,model.trainable_variables)求出模 型中所有的梯度信息\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial{\\theta}},\\theta\\in\\{W_1,b_1,W_2,b_2,W_3,b_3\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2047849\n"
     ]
    }
   ],
   "source": [
    "#Step3.计算参数的梯度\n",
    "grads=tape.gradient(loss,model.trainable_variables)\n",
    "#w'=w-lr*grad,更新参数网络\n",
    "optimizer=optimizers.SGD(learning_rate=0.001)\n",
    "optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "print(loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    # Step4.loop\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:#构建梯度记录环境\n",
    "            #打平,[b,28,28]-->[b,784]\n",
    "            x=tf.reshape(x,(-1,28*28))\n",
    "            #Step1.得到模型输出Output\n",
    "            #[b,784]->[b,10]\n",
    "            out=model(x)\n",
    "            loss=tf.reduce_sum(tf.square(out-y))/x.shape[0]\n",
    "        # w' = w - lr * grad\n",
    "        grads=tape.gradient(loss,model.trainable_variables)\n",
    "        #w'=w-lr*grad,更新参数网络\n",
    "        optimizer=optimizers.SGD(learning_rate=0.001)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(60):\n",
    "        train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 5.1850233\n",
      "0 100 loss: 1.0177236\n",
      "1 0 loss: 0.9137496\n",
      "1 100 loss: 0.7905963\n",
      "2 0 loss: 0.7397393\n",
      "2 100 loss: 0.69073355\n",
      "3 0 loss: 0.65821636\n",
      "3 100 loss: 0.63227177\n",
      "4 0 loss: 0.6076087\n",
      "4 100 loss: 0.59161043\n",
      "5 0 loss: 0.5717077\n",
      "5 100 loss: 0.5607679\n",
      "6 0 loss: 0.54404736\n",
      "6 100 loss: 0.53585154\n",
      "7 0 loss: 0.52118975\n",
      "7 100 loss: 0.51502055\n",
      "8 0 loss: 0.50194556\n",
      "8 100 loss: 0.4973259\n",
      "9 0 loss: 0.4851991\n",
      "9 100 loss: 0.48177868\n",
      "10 0 loss: 0.4705286\n",
      "10 100 loss: 0.46769357\n",
      "11 0 loss: 0.45751068\n",
      "11 100 loss: 0.45512518\n",
      "12 0 loss: 0.44585454\n",
      "12 100 loss: 0.44370478\n",
      "13 0 loss: 0.4354513\n",
      "13 100 loss: 0.43330032\n",
      "14 0 loss: 0.42601967\n",
      "14 100 loss: 0.42374143\n",
      "15 0 loss: 0.4175372\n",
      "15 100 loss: 0.41496474\n",
      "16 0 loss: 0.4096306\n",
      "16 100 loss: 0.40688446\n",
      "17 0 loss: 0.4021516\n",
      "17 100 loss: 0.39937186\n",
      "18 0 loss: 0.39511353\n",
      "18 100 loss: 0.3924002\n",
      "19 0 loss: 0.3885798\n",
      "19 100 loss: 0.38590145\n",
      "20 0 loss: 0.38240317\n",
      "20 100 loss: 0.37980312\n",
      "21 0 loss: 0.37662542\n",
      "21 100 loss: 0.37409207\n",
      "22 0 loss: 0.37125897\n",
      "22 100 loss: 0.36871785\n",
      "23 0 loss: 0.3661843\n",
      "23 100 loss: 0.3635944\n",
      "24 0 loss: 0.36145067\n",
      "24 100 loss: 0.35875857\n",
      "25 0 loss: 0.35690758\n",
      "25 100 loss: 0.35409898\n",
      "26 0 loss: 0.35254803\n",
      "26 100 loss: 0.34968257\n",
      "27 0 loss: 0.34841132\n",
      "27 100 loss: 0.34546393\n",
      "28 0 loss: 0.34448057\n",
      "28 100 loss: 0.3414091\n",
      "29 0 loss: 0.34068704\n",
      "29 100 loss: 0.33753127\n",
      "30 0 loss: 0.33702797\n",
      "30 100 loss: 0.3337968\n",
      "31 0 loss: 0.33353707\n",
      "31 100 loss: 0.33022034\n",
      "32 0 loss: 0.3301606\n",
      "32 100 loss: 0.32674807\n",
      "33 0 loss: 0.32694232\n",
      "33 100 loss: 0.3234169\n",
      "34 0 loss: 0.32381108\n",
      "34 100 loss: 0.3201998\n",
      "35 0 loss: 0.3208615\n",
      "35 100 loss: 0.3170948\n",
      "36 0 loss: 0.3179856\n",
      "36 100 loss: 0.31406754\n",
      "37 0 loss: 0.3151998\n",
      "37 100 loss: 0.31113464\n",
      "38 0 loss: 0.31252038\n",
      "38 100 loss: 0.30834588\n",
      "39 0 loss: 0.30990702\n",
      "39 100 loss: 0.30560565\n",
      "40 0 loss: 0.30738938\n",
      "40 100 loss: 0.3029562\n",
      "41 0 loss: 0.3049288\n",
      "41 100 loss: 0.30037254\n",
      "42 0 loss: 0.30254206\n",
      "42 100 loss: 0.29786068\n",
      "43 0 loss: 0.30025128\n",
      "43 100 loss: 0.2954474\n",
      "44 0 loss: 0.2980063\n",
      "44 100 loss: 0.29310888\n",
      "45 0 loss: 0.29579622\n",
      "45 100 loss: 0.29082307\n",
      "46 0 loss: 0.2936397\n",
      "46 100 loss: 0.2886269\n",
      "47 0 loss: 0.291533\n",
      "47 100 loss: 0.28644955\n",
      "48 0 loss: 0.2894959\n",
      "48 100 loss: 0.28431213\n",
      "49 0 loss: 0.28749728\n",
      "49 100 loss: 0.28222376\n",
      "50 0 loss: 0.28555524\n",
      "50 100 loss: 0.28018755\n",
      "51 0 loss: 0.28368396\n",
      "51 100 loss: 0.27820766\n",
      "52 0 loss: 0.28182366\n",
      "52 100 loss: 0.2762696\n",
      "53 0 loss: 0.28001565\n",
      "53 100 loss: 0.2743895\n",
      "54 0 loss: 0.27824247\n",
      "54 100 loss: 0.272551\n",
      "55 0 loss: 0.2764987\n",
      "55 100 loss: 0.2707498\n",
      "56 0 loss: 0.27482423\n",
      "56 100 loss: 0.26897562\n",
      "57 0 loss: 0.27318043\n",
      "57 100 loss: 0.2672627\n",
      "58 0 loss: 0.2716011\n",
      "58 100 loss: 0.2656194\n",
      "59 0 loss: 0.27006072\n",
      "59 100 loss: 0.26399502\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01827951 -0.12186617  0.7284543   0.09377631  0.02286693  0.06474251\n",
      "   0.07318782 -0.06055652 -0.00634248 -0.09883674]], shape=(1, 10), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_valt=2*tf.convert_to_tensor(x_val,dtype=tf.float32)/255-1\n",
    "testx=tf.reshape(x_valt[731],(-1,28*28))\n",
    "result=model(testx)\n",
    "print(result)\n",
    "print(tf.argmax(result,axis=1))\n",
    "imag=x_val[731]\n",
    "plt.imshow(imag)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(float, tensorflow.python.framework.ops.EagerTensor, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a=1.2\n",
    "aa=tf.constant(1.2)\n",
    "type(a),type(aa),tf.is_tensor(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1. , 2. , 3.3], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.constant([1,2.,3.3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便导出数据和其他的系统模块，使用numpy()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 2. , 3.3], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与标量不同，向量的定义需通过List类型传给tf.constant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.2], dtype=float32)>,\n",
       " TensorShape([1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([1.2])\n",
    "a,a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义矩阵时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       " array([[1, 2],\n",
       "        [3, 4]])>,\n",
       " TensorShape([2, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([[1,2],[3,4]])\n",
    "a,a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3维的张量可以定义为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
       " array([[[1, 2],\n",
       "         [3, 4]],\n",
       " \n",
       "        [[5, 6],\n",
       "         [7, 8]]])>,\n",
       " TensorShape([2, 2, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "a,a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字符串布尔型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello,Deep Learning.'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant('Hello,Deep Learning.')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello,deep learning.'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.lower(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant(True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([True,False])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意TensorFlow的布尔类型和python语言的布尔类型不对等：(但是测试结果好像ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant(True)\n",
    "print(a==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数值精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-13035, shape=(), dtype=int16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=123456789>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.constant(123456789,dtype=tf.int16))\n",
    "tf.constant(123456789,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存精度过低时，数据发生了溢出，得到了错误的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.1415927>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.pi)\n",
    "tf.constant(np.pi,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以采用tf.float64来保存$\\pi$能获得更高的精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.141592653589793>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.pi,dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: <dtype: 'bool'>\n",
      "after: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print('before:',a.dtype)\n",
    "if a.dtype!=tf.float32:\n",
    "    a=tf.cast(a,tf.float32)\n",
    "print('after:',a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.140625>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant(np.pi,dtype=tf.float16)\n",
    "tf.cast(a,tf.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高精度向低精度转化时可能发生数据溢出隐患"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int16, numpy=-13035>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant(123456789,dtype=tf.int32)\n",
    "tf.cast(a,tf.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "布尔型和整形之间也可以互相转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 0])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([True,False])\n",
    "tf.cast(a,tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False,  True,  True])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([-1,0,1,2])\n",
    "tf.cast(a,tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow常用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable可以将普通的张量转化为待优化的张量，从而可以跟踪相关梯度信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Variable:0', True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.constant([-1,0,1,2])\n",
    "aa=tf.Variable(a)\n",
    "aa.name,aa.trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以直接使用Variable来创建张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tf.Variable([[1,2],[3,4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，我们需要把数据的特征和标签一一配对后喂入网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "features=tf.constant([12,23,10,17])\n",
    "labels=tf.constant([0,1,1,0])\n",
    "dataset=tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "print(dataset)\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对参数求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    w=tf.Variable(tf.constant(3.0))\n",
    "    loss=tf.pow(w,2)\n",
    "grad=tape.gradient(loss,w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate常在for循环中使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "seq=['one','two','three']\n",
    "for i,element in enumerate(seq):\n",
    "    print(i,element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类问题中，常用独热码做标签，1表示是，0表示不是，取值$[0,1]$之间可以表示是的概率是多少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用tf.one_hot函数可以把待转换的数据转化为one-hot形式的数据输出格式为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.one_hot(带转化数据，depth=几分类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classes=3\n",
    "labels=tf.constant([1,0,2])\n",
    "output=tf.one_hot(labels,depth=classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，需要通过变化来表示属于某一类的概率，变换公式如下，使n个分类的n个输出符合概率分布\n",
    "$$\n",
    "Softmax(y_i)=\\frac{e^{y_i}}{\\sum_{j=0}^ne^{y_i}}\\\\\n",
    "\\forall x, P(X=x)\\in[0,1]\\quad\\sum_xP(X=x)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After softmax,y_pro is tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y=tf.constant([1.01,2.01,-0.66])\n",
    "y_pro=tf.nn.softmax(y)\n",
    "print('After softmax,y_pro is',y_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数自更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新参数的值并返回，但先用tf.Variable定义变量为可训练，下面是自减的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "w=tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量最大值索引号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test=np.array([[1,2,3],[2,3,4],[5,4,3],[8,7,2]])\n",
    "print(test)\n",
    "print(tf.argmax(test,axis=0))\n",
    "print(tf.argmax(test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 鸢尾花数据集的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data_add_index: \n",
      "      花萼长度  花儿宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_data=datasets.load_iris().data\n",
    "y_data=datasets.load_iris().target\n",
    "#为了让数据更好看我们采用pandas进行整理\n",
    "x_data=DataFrame(x_data,columns=['花萼长度','花儿宽度','花瓣长度','花瓣宽度'])\n",
    "pd.set_option('display.unicode.east_asian_width',True)#设置列名对齐\n",
    "print('x_data_add_index: \\n',x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data add a column:\n",
      "      花萼长度  花儿宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "x_data['类别']=y_data\n",
    "print('x_data add a column:\\n',x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 鸢尾花分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集，打乱并分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data=datasets.load_iris().data\n",
    "y_data=datasets.load_iris().target\n",
    "#打乱数据集\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "#数据集分为训练集和测试集\n",
    "x_train=x_data[:-30]\n",
    "y_train=y_data[:-30]\n",
    "x_test=x_data[-30:]\n",
    "y_test=y_data[-30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配对成输入特征和标签对之前，先对数据类型进行转换，避免因为矩阵相乘时数据类型不一致报错,打包（batch），每次喂入一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tf.cast(x_train,tf.float32)\n",
    "x_test=tf.cast(x_test,tf.float32)\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们采用一层的神经网络(输入4个特征，输出四种花)，并定义其中所有的可训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=tf.Variable(tf.random.truncated_normal([4,3],stddev=0.1,seed=1))\n",
    "b1=tf.Variable(tf.random.truncated_normal([3],stddev=0.1,seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义学习率，记录loss和acc（准确率）使用的list，epoch，loss_all记录四个step生成的4个loss的和（128/batch）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.1\n",
    "train_loss_results=[]\n",
    "test_acc=[]\n",
    "epoch=500\n",
    "loss_all=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss:0.022661410504952073\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 1, loss:0.022657548310235143\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 2, loss:0.02265369798988104\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 3, loss:0.022649836027994752\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 4, loss:0.022646001307293773\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 5, loss:0.022642147494480014\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 6, loss:0.02263831067830324\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 7, loss:0.02263447572477162\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 8, loss:0.022630640072748065\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 9, loss:0.022626824444159865\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 10, loss:0.02262299507856369\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 11, loss:0.02261917549185455\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 12, loss:0.022615368478000164\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 13, loss:0.022611547959968448\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 14, loss:0.022607748862355947\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 15, loss:0.022603941150009632\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 16, loss:0.022600154858082533\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 17, loss:0.02259635366499424\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 18, loss:0.022592563647776842\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 19, loss:0.022588782478123903\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 20, loss:0.022584994323551655\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 21, loss:0.02258121594786644\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 22, loss:0.022577438969165087\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 23, loss:0.022573674097657204\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 24, loss:0.02256991364993155\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 25, loss:0.02256614505313337\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 26, loss:0.022562386002391577\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 27, loss:0.022558635333552957\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 28, loss:0.02255489001981914\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 29, loss:0.022551138186827302\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 30, loss:0.022547407541424036\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 31, loss:0.022543659433722496\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 32, loss:0.022539931116625667\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 33, loss:0.02253619907423854\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 34, loss:0.02253247215412557\n",
      "Test.acc: 0.9666666666666667\n",
      "---------------------------------------\n",
      "Epoch 35, loss:0.022528750589117408\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 36, loss:0.02252503577619791\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 37, loss:0.022521323058754206\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 38, loss:0.0225176140666008\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 39, loss:0.022513907635584474\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 40, loss:0.022510211914777756\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 41, loss:0.022506504552438855\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 42, loss:0.022502816282212734\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 43, loss:0.02249912661500275\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 44, loss:0.022495434619486332\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 45, loss:0.022491754731163383\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 46, loss:0.022488076239824295\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 47, loss:0.022484405199065804\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 48, loss:0.022480728570371866\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 49, loss:0.02247706218622625\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 50, loss:0.022473401855677366\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 51, loss:0.022469737101346254\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 52, loss:0.02246608678251505\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 53, loss:0.022462432039901614\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 54, loss:0.02245879382826388\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 55, loss:0.0224551425781101\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 56, loss:0.022451501805335283\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 57, loss:0.022447869647294283\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 58, loss:0.022444237722083926\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 59, loss:0.022440605331212282\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 60, loss:0.022436979226768017\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 61, loss:0.022433358011767268\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 62, loss:0.02242973749525845\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 63, loss:0.022426138864830136\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 64, loss:0.022422515554353595\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 65, loss:0.02241891622543335\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 66, loss:0.022415307350456715\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 67, loss:0.02241170941852033\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 68, loss:0.022408118937164545\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 69, loss:0.022404528222978115\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 70, loss:0.022400941466912627\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 71, loss:0.022397360997274518\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 72, loss:0.02239377098158002\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 73, loss:0.02239020448178053\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 74, loss:0.022386622615158558\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 75, loss:0.022383059607818723\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 76, loss:0.022379493340849876\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 77, loss:0.02237593405880034\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 78, loss:0.022372378036379814\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 79, loss:0.022368817124515772\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 80, loss:0.02236526715569198\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 81, loss:0.022361723706126213\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 82, loss:0.022358172573149204\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 83, loss:0.02235464146360755\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 84, loss:0.02235110756009817\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 85, loss:0.022347572492435575\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 86, loss:0.022344046737998724\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 87, loss:0.022340521216392517\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 88, loss:0.022336996626108885\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, loss:0.022333484143018723\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 90, loss:0.022329967003315687\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 91, loss:0.02232645731419325\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 92, loss:0.02232294948771596\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 93, loss:0.022319449577480555\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 94, loss:0.022315953392535448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 95, loss:0.022312455344945192\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 96, loss:0.022308964282274246\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 97, loss:0.022305469727143645\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 98, loss:0.02230198634788394\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 99, loss:0.02229850529693067\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 100, loss:0.022295017959550023\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 101, loss:0.022291548550128937\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 102, loss:0.02228807657957077\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 103, loss:0.022284609265625477\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 104, loss:0.0222811468411237\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 105, loss:0.022277683950960636\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 106, loss:0.022274226183071733\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 107, loss:0.022270771907642484\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 108, loss:0.022267320891842246\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 109, loss:0.022263873601332307\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 110, loss:0.022260422352701426\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 111, loss:0.02225698996335268\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 112, loss:0.022253553848713636\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 113, loss:0.022250116802752018\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 114, loss:0.02224668674170971\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 115, loss:0.02224326180294156\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 116, loss:0.02223984245210886\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 117, loss:0.022236421471461654\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 118, loss:0.022233002353459597\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 119, loss:0.02222958765923977\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 120, loss:0.02222617552615702\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 121, loss:0.022222769679501653\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 122, loss:0.022219373378902674\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 123, loss:0.022215974982827902\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 124, loss:0.022212580777704716\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 125, loss:0.02220918540842831\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 126, loss:0.02220579097047448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 127, loss:0.02220241678878665\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 128, loss:0.022199030499905348\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 129, loss:0.022195648634806275\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 130, loss:0.022192276315763593\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 131, loss:0.022188906325027347\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 132, loss:0.022185529815033078\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 133, loss:0.022182168904691935\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 134, loss:0.02217879774980247\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 135, loss:0.022175445687025785\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 136, loss:0.022172088734805584\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 137, loss:0.022168738767504692\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 138, loss:0.022165389731526375\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 139, loss:0.022162042558193207\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 140, loss:0.02215870190411806\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 141, loss:0.022155363578349352\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 142, loss:0.022152028512209654\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 143, loss:0.022148688789457083\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 144, loss:0.02214536420069635\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 145, loss:0.022142038214951754\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 146, loss:0.02213871804997325\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 147, loss:0.022135399049147964\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 148, loss:0.022132085636258125\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 149, loss:0.02212876221165061\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 150, loss:0.022125458577647805\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 151, loss:0.022122154477983713\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 152, loss:0.022118846885859966\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 153, loss:0.022115549072623253\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 154, loss:0.022112254053354263\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 155, loss:0.022108955308794975\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 156, loss:0.02210566191934049\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 157, loss:0.022102378075942397\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 158, loss:0.022099097026512027\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 159, loss:0.02209581551142037\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 160, loss:0.022092539118602872\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 161, loss:0.022089264821261168\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 162, loss:0.02208599424920976\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 163, loss:0.02208272647112608\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 164, loss:0.022079454734921455\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 165, loss:0.022076203022152185\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 166, loss:0.022072938038036227\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 167, loss:0.022069682367146015\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 168, loss:0.022066433215513825\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 169, loss:0.02206317870877683\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 170, loss:0.022059932816773653\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 171, loss:0.022056682966649532\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 172, loss:0.02205345337279141\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 173, loss:0.022050212137401104\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 174, loss:0.022046978352591395\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 175, loss:0.02204375248402357\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 176, loss:0.02204052498564124\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 177, loss:0.022037306101992726\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 178, loss:0.022034075343981385\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 179, loss:0.02203085832297802\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 180, loss:0.022027643863111734\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 181, loss:0.02202442637644708\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 182, loss:0.022021224023774266\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, loss:0.02201801724731922\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 184, loss:0.022014812100678682\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 185, loss:0.022011621156707406\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 186, loss:0.022008419269695878\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 187, loss:0.02200522320345044\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 188, loss:0.02200203645043075\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 189, loss:0.021998848533257842\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 190, loss:0.02199565782211721\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 191, loss:0.021992476424202323\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 192, loss:0.021989301312714815\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 193, loss:0.02198612759821117\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 194, loss:0.021982949459925294\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 195, loss:0.02197978156618774\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 196, loss:0.02197660831734538\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 197, loss:0.021973453694954515\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 198, loss:0.0219702972099185\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 199, loss:0.021967126056551933\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 200, loss:0.021963976556435227\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 201, loss:0.021960829151794314\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 202, loss:0.021957674762234092\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 203, loss:0.021954528987407684\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 204, loss:0.02195138786919415\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 205, loss:0.021948246052488685\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 206, loss:0.021945116808637977\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 207, loss:0.02194198127835989\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 208, loss:0.02193884691223502\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 209, loss:0.021935714641585946\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 210, loss:0.02193258935585618\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 211, loss:0.02192947268486023\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 212, loss:0.02192635554820299\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 213, loss:0.021923233522102237\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 214, loss:0.021920115454122424\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 215, loss:0.021917011821642518\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 216, loss:0.02191390097141266\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 217, loss:0.021910799900069833\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 218, loss:0.021907692775130272\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 219, loss:0.021904599154368043\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 220, loss:0.021901501342654228\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 221, loss:0.021898408653214574\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 222, loss:0.021895320154726505\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 223, loss:0.021892227698117495\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 224, loss:0.02188915037550032\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 225, loss:0.02188606234267354\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 226, loss:0.021882982924580574\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 227, loss:0.021879905136302114\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 228, loss:0.021876836894080043\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 229, loss:0.02187376841902733\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 230, loss:0.021870690397918224\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 231, loss:0.021867631003260612\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 232, loss:0.021864571142941713\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 233, loss:0.021861511282622814\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 234, loss:0.02185845375061035\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 235, loss:0.021855395287275314\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 236, loss:0.021852343576028943\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 237, loss:0.021849300246685743\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 238, loss:0.021846248768270016\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 239, loss:0.021843216381967068\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 240, loss:0.021840181667357683\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 241, loss:0.021837140200659633\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 242, loss:0.021834099432453513\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 243, loss:0.021831079619005322\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 244, loss:0.021828047232702374\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 245, loss:0.021825011586770415\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 246, loss:0.021821997361257672\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 247, loss:0.021818977082148194\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 248, loss:0.021815950516611338\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 249, loss:0.02181294816546142\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 250, loss:0.021809930447489023\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 251, loss:0.02180692134425044\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 252, loss:0.02180391689762473\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 253, loss:0.021800919203087687\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 254, loss:0.02179791289381683\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 255, loss:0.021794913336634636\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 256, loss:0.02179192123003304\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 257, loss:0.021788921440020204\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 258, loss:0.02178593771532178\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 259, loss:0.021782946307212114\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 260, loss:0.021779966773465276\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 261, loss:0.02177697722800076\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 262, loss:0.021774005377665162\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 263, loss:0.021771031199023128\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 264, loss:0.021768048172816634\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 265, loss:0.02176508354023099\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 266, loss:0.021762109827250242\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 267, loss:0.021759145194664598\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 268, loss:0.02175618475303054\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 269, loss:0.02175322105176747\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 270, loss:0.021750260144472122\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 271, loss:0.021747311810031533\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 272, loss:0.021744364639744163\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 273, loss:0.02174140582792461\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274, loss:0.021738458424806595\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 275, loss:0.021735518239438534\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 276, loss:0.0217325733974576\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 277, loss:0.021729636006057262\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 278, loss:0.02172670396976173\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 279, loss:0.021723762853071094\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 280, loss:0.021720836171880364\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 281, loss:0.021717899711802602\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 282, loss:0.021714983973652124\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 283, loss:0.021712057292461395\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 284, loss:0.021709123859182\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 285, loss:0.021706222090870142\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 286, loss:0.021703302627429366\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 287, loss:0.021700382931157947\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 288, loss:0.0216974806971848\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 289, loss:0.02169455890543759\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 290, loss:0.021691657369956374\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 291, loss:0.02168876468203962\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 292, loss:0.0216858615167439\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 293, loss:0.02168296044692397\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 294, loss:0.02168006612919271\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 295, loss:0.021677170414477587\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 296, loss:0.021674292627722025\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 297, loss:0.021671395748853683\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 298, loss:0.02166851586662233\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 299, loss:0.021665635518729687\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 300, loss:0.021662743762135506\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 301, loss:0.021659878781065345\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 302, loss:0.021657000994309783\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 303, loss:0.021654128096997738\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 304, loss:0.021651262417435646\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 305, loss:0.021648391615599394\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 306, loss:0.021645533153787255\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 307, loss:0.02164266398176551\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 308, loss:0.021639808313921094\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 309, loss:0.02163694892078638\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 310, loss:0.02163408836349845\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 311, loss:0.02163124899379909\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 312, loss:0.02162839425727725\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 313, loss:0.02162554720416665\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 314, loss:0.021622693864628673\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 315, loss:0.021619864040985703\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 316, loss:0.021617020713165402\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 317, loss:0.02161418180912733\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 318, loss:0.02161134802736342\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 319, loss:0.021608520997688174\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 320, loss:0.02160569210536778\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 321, loss:0.021602855063974857\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 322, loss:0.02160003036260605\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 323, loss:0.02159720566123724\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 324, loss:0.021594395162537694\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 325, loss:0.021591576281934977\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 326, loss:0.02158876322209835\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 327, loss:0.021585950627923012\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 328, loss:0.021583136403933167\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 329, loss:0.021580325439572334\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 330, loss:0.02157752332277596\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 331, loss:0.02157472725957632\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 332, loss:0.021571923047304153\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 333, loss:0.021569127682596445\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 334, loss:0.021566328592598438\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 335, loss:0.02156353904865682\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 336, loss:0.02156074345111847\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 337, loss:0.02155795763246715\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 338, loss:0.021555163897573948\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 339, loss:0.02155238389968872\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 340, loss:0.021549604134634137\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 341, loss:0.021546820178627968\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 342, loss:0.021544050658121705\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 343, loss:0.021541276015341282\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 344, loss:0.02153849950991571\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 345, loss:0.021535736974328756\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 346, loss:0.021532967686653137\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 347, loss:0.021530203986912966\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 348, loss:0.021527437027543783\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 349, loss:0.02152468217536807\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 350, loss:0.021521923132240772\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 351, loss:0.021519165951758623\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 352, loss:0.021516418317332864\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 353, loss:0.021513668820261955\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 354, loss:0.021510915830731392\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 355, loss:0.02150817308574915\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 356, loss:0.021505428245291114\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 357, loss:0.021502690389752388\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 358, loss:0.021499956725165248\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 359, loss:0.02149721374735236\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 360, loss:0.021494474029168487\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 361, loss:0.021491752238944173\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 362, loss:0.021489021368324757\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363, loss:0.021486292826011777\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 364, loss:0.021483566612005234\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 365, loss:0.02148083969950676\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 366, loss:0.021478121867403388\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 367, loss:0.02147540869191289\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 368, loss:0.021472694585099816\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 369, loss:0.021469969768077135\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 370, loss:0.021467271028086543\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 371, loss:0.0214645576197654\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 372, loss:0.02146184747107327\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 373, loss:0.021459147334098816\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 374, loss:0.021456440910696983\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 375, loss:0.021453747525811195\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 376, loss:0.02145105367526412\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 377, loss:0.021448350278660655\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 378, loss:0.021445657592266798\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 379, loss:0.02144296863116324\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 380, loss:0.021440279204398394\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 381, loss:0.021437600953504443\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 382, loss:0.021434909664094448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 383, loss:0.021432229317724705\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 384, loss:0.021429552463814616\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 385, loss:0.02142687002196908\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 386, loss:0.0214242001529783\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 387, loss:0.02142152562737465\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 388, loss:0.021418849239125848\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 389, loss:0.021416185656562448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 390, loss:0.02141352230682969\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 391, loss:0.021410860121250153\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 392, loss:0.021408200031146407\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 393, loss:0.021405529463663697\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 394, loss:0.021402886835858226\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 395, loss:0.0214002279099077\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 396, loss:0.02139756246469915\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 397, loss:0.02139491541311145\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 398, loss:0.02139227162115276\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 399, loss:0.02138962014578283\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 400, loss:0.021386975422501564\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 401, loss:0.021384334657341242\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 402, loss:0.021381701342761517\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 403, loss:0.021379057317972183\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 404, loss:0.021376437041908503\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 405, loss:0.021373793249949813\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 406, loss:0.02137115760706365\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 407, loss:0.02136853034608066\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 408, loss:0.0213659037835896\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 409, loss:0.02136328350752592\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 410, loss:0.021360659506171942\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 411, loss:0.021358033176511526\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 412, loss:0.021355427335947752\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 413, loss:0.021352801006287336\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 414, loss:0.02135019050911069\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 415, loss:0.02134757535532117\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 416, loss:0.021344973472878337\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 417, loss:0.021342361578717828\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 418, loss:0.021339762257412076\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 419, loss:0.021337145706638694\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 420, loss:0.02133454568684101\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 421, loss:0.021331951953470707\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 422, loss:0.021329355193302035\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 423, loss:0.0213267607614398\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 424, loss:0.02132416609674692\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 425, loss:0.021321583539247513\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 426, loss:0.021318990970030427\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 427, loss:0.021316402358934283\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 428, loss:0.0213138188701123\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 429, loss:0.021311242831870914\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 430, loss:0.02130866516381502\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 431, loss:0.021306080976501107\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 432, loss:0.021303500048816204\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 433, loss:0.021300927503034472\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 434, loss:0.021298362175002694\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 435, loss:0.02129579009488225\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 436, loss:0.021293228026479483\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 437, loss:0.021290653850883245\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 438, loss:0.021288098767399788\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 439, loss:0.02128553087823093\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 440, loss:0.021282974164932966\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 441, loss:0.021280414424836636\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 442, loss:0.02127786772325635\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 443, loss:0.021275305654853582\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 444, loss:0.02127276104874909\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 445, loss:0.021270210621878505\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 446, loss:0.021267668344080448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 447, loss:0.021265114890411496\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 448, loss:0.021262577502056956\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 449, loss:0.021260035457089543\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 450, loss:0.021257495507597923\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 451, loss:0.02125496487133205\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 452, loss:0.02125242236070335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 453, loss:0.021249890327453613\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 454, loss:0.021247357595711946\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 455, loss:0.02124483766965568\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 456, loss:0.02124231099151075\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 457, loss:0.0212397831492126\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 458, loss:0.021237262757495046\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 459, loss:0.02123473258689046\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 460, loss:0.021232223603874445\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 461, loss:0.02122970507480204\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 462, loss:0.021227189106866717\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 463, loss:0.021224682917818427\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 464, loss:0.021222161129117012\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 465, loss:0.02121965866535902\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 466, loss:0.021217157365754247\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 467, loss:0.02121465397067368\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 468, loss:0.021212144754827023\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 469, loss:0.02120963577181101\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 470, loss:0.021207147045060992\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 471, loss:0.021204650169238448\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 472, loss:0.021202160511165857\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 473, loss:0.021199671551585197\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 474, loss:0.02119717374444008\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 475, loss:0.021194682456552982\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 476, loss:0.021192197687923908\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 477, loss:0.02118971128948033\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 478, loss:0.021187230944633484\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 479, loss:0.021184750366955996\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 480, loss:0.02118227258324623\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 481, loss:0.021179792005568743\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 482, loss:0.021177322836592793\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 483, loss:0.021174848079681396\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 484, loss:0.021172379376366735\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 485, loss:0.021169905783608556\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 486, loss:0.021167442901059985\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 487, loss:0.021164973499253392\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 488, loss:0.02116250991821289\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 489, loss:0.02116005215793848\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 490, loss:0.021157590206712484\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 491, loss:0.02115513593889773\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 492, loss:0.02115268027409911\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 493, loss:0.02115022041834891\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 494, loss:0.02114776987582445\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 495, loss:0.021145330276340246\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n",
      "Epoch 496, loss:0.021142879966646433\n",
      "Test.acc: 0.975\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):\n",
    "    for step,(x_train,y_train)in enumerate(train_db):\n",
    "        with tf.GradientTape()as tape:\n",
    "            y=tf.matmul(x_train,w1)+b1\n",
    "            y=tf.nn.softmax(y)\n",
    "            y_=tf.one_hot(y_train,depth=3)#标签转化为one-hot编码可以和softmax的概率值相减求loss#求均方误差\n",
    "            loss=tf.reduce_mean(tf.square(y_-y))#求均方误差\n",
    "            loss_all=loss_all+loss.numpy()\n",
    "        grads=tape.gradient(loss,[w1,b1])\n",
    "        #梯度下降更新参数\n",
    "        w1.assign_sub(lr*grads[0])\n",
    "        b1.assign_sub(lr*grads[1])\n",
    "    print('Epoch {}, loss:{}'.format(epoch,loss_all/4))#平均每个step的误差\n",
    "    train_loss_results.append(loss_all/4)\n",
    "    loss_all=0#loss_all归零为下一个epoch的loss做准备\n",
    "    \n",
    "    #计算准确率\n",
    "    total_correct,total_number=0,0\n",
    "    for x_test,y_test in test_db:\n",
    "        y=tf.matmul(x_test,w1)+b1\n",
    "        y=tf.nn.softmax(y)\n",
    "        pred=tf.argmax(y,axis=1)\n",
    "        pred=tf.cast(pred,dtype=y_test.dtype)\n",
    "        correct=tf.cast(tf.equal(pred,y_test),dtype=tf.int32)\n",
    "        correct=tf.reduce_sum(correct)\n",
    "        total_correct+=int(correct)\n",
    "        total_number+=x_test.shape[0]\n",
    "    acc=total_correct/total_number\n",
    "    test_acc.append(acc)\n",
    "    print('Test.acc:',acc)\n",
    "    print('---------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一部分用来测试准确率，画图绘制loss和acc曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('loss and acc function curve')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss and epoch')\n",
    "plt.plot(train_loss_results,label='$loss$')\n",
    "plt.plot(test_acc,label='$acc$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 函数扫盲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where函数如果条件是真返回A，条件是假返回B。tf.where(条件,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a=tf.constant([1,2,3,1,1])\n",
    "b=tf.constant([0,1,3,4,5])\n",
    "c=tf.where(tf.greater(a,b),a,b)#若a>b,返回a对应位置的元素，否则返回b对应为位置的函数\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.RandomState.rand(维度)返回一个$[0,1)$区间的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.417022004702574\n",
      "b [[7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      " [1.46755891e-01 9.23385948e-02 1.86260211e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rdm=np.random.RandomState(seed=1)\n",
    "a=rdm.rand()\n",
    "b=rdm.rand(2,3)\n",
    "print('a',a)\n",
    "print('b',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.vstack()可以将两个数组按垂直方向叠加np.vstack(数组1,数组2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,2,3])\n",
    "b=np.array([4,5,6])\n",
    "c=np.vstack((a,b))\n",
    "print('c\\n',c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.mgrid[起始值：结束值：步长，起始值：结束值：步长，...]返回若干组维度相同的等差数组,两个返回值的维度都相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.ravel()把x变为一维数组，拉直"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.c_[]使返回的间隔数值点配对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [[1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]]\n",
      "y [[2.  2.5 3.  3.5]\n",
      " [2.  2.5 3.  3.5]]\n",
      "grid:\n",
      " [[1.  2. ]\n",
      " [1.  2.5]\n",
      " [1.  3. ]\n",
      " [1.  3.5]\n",
      " [2.  2. ]\n",
      " [2.  2.5]\n",
      " [2.  3. ]\n",
      " [2.  3.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x,y=np.mgrid[1:3:1,2:4:0.5]\n",
    "grid=np.c_[x.ravel(),y.ravel()]\n",
    "print('x',x)\n",
    "print('y',y)\n",
    "print('grid:\\n',grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络复杂度和激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层数为隐藏层+输出层，输入层不做计算，时间复杂度按照乘加运算的次数来决定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提高多层数神经网络的表达能力，在多层神经网络中加入非线性激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优秀的激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 非线性：激活函数非线性时，多层神经网络可逼近所有函数\n",
    "- 可微性：优化器大多使用梯度下降更新参数\n",
    "- 单调性：当激活函数单调时，能保证单层网络的损失函数是凸函数\n",
    "- 近似恒等性：$f(x)\\sim x$当参数初始化为随机小值时，神经网络更稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数输出值的范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 激活函数输出为有限值时，基于梯度的优化方法更稳定\n",
    "- 激活函数输出为无限值时，调校学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.linspace(-10,10,1000)\n",
    "y=1/(1+np.exp(-x))\n",
    "y1=np.exp(-x)/(1+np.exp(-x))**2\n",
    "plt.plot(x,y,label='original')\n",
    "plt.plot(x,y1,label='diff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 易造成梯度消失（导数范围在$[0,0.25]$之间，多层链式求导导致导数值变小）\n",
    "- 输出非0均值，收敛慢\n",
    "- 幂运算复杂，训练时间长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f=(1-np.exp(-2*x))/(1+np.exp(-2*x))\n",
    "plt.plot(x,f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出时0均值\n",
    "- 易造成梯度消失\n",
    "- 幂运算复杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\max(x,0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu=x*(x>0)\n",
    "plt.plot(x,relu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 解决了梯度消失问题（在正区间）\n",
    "- 只需要判断是否大于0，计算速度快\n",
    "- 收敛速度快于sigmoid和tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输出非0均值，收敛满\n",
    "- 某些神经元可能永远不会被激活，参数永远不能被更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\max(\\alpha x,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=0.1\n",
    "leaky_relu=alpha*x*(x<0)+x*(x>0)\n",
    "plt.plot(x,leaky_relu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从理论上讲，Leaky Relu有Relu的所有优点，外加不会有Dead Relu问题，并没有完全证明Leaky Relu总是好于Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对于初学者（我）的建议"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 首选Relu激活函数\n",
    "- 学习率设置较小值\n",
    "- 输入特征标准化，让输入特征满足以0为均值，1为标准差的正态分布\n",
    "- 初始参数中心化，即让随机生成的参数满足以0为均值，$\\sqrt{\\frac{2}{\\text{当前层输入特征个数}}}$为标准差的正态分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设y是预测值，y_是已知答案\n",
    "$$\n",
    "MSE(y\\_,y)=\\frac{\\sum_{i-1}^n(y-y\\_)^2}{n}\n",
    "$$\n",
    "lose_mse=tf.reduce_mean(tf.square(y-y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有些情况预测多了和预测少了得到的损失不同，因此我们需要用分段函数来定义损失函数\n",
    "$$\n",
    "f(y\\_,y)=\\begin{cases}\n",
    "profit*(y\\_-y)&y<y\\_\\\\\n",
    "cost*(y-y\\_)&y\\geq y\\_\n",
    "\\end{cases}\n",
    "$$\n",
    "loss_zdy=tf.reduce_sum(tf.where(tf.greater(y,y_),cost*(y-y_),profit*(y_-y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数CE（Cross Entropy）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表征两个概率分布之间的距离\n",
    "$$\n",
    "H(y\\_,y)=-\\sum y\\_*\\ln y\n",
    "$$\n",
    "tf.losses.categorical_crossentropy(y_,y)\n",
    "<p> tf.nn.softmax_cross_entropy_with_logits(y_,y)\n",
    "<p> 可先将y转化为softmax编码，然后求交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 缓解过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化缓解过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化在损失函数中引入模型复杂度指标，给w加权值，弱化了训练数据的噪声\n",
    "$$\n",
    "loss=loss(y,y\\_)+regularizer*loss(w)\n",
    "$$\n",
    "其中loss(w)需要进行正则化，\n",
    "$$\n",
    "loss_{L1}(w)=\\sum_i|w_i|\\\\\n",
    "loss_{L2}(w)=\\sum_i|w_i^2|\n",
    "$$\n",
    "- L1正则化大概率会是很多参数变为零，通过稀疏参数，减少参数数量，降低复杂度\n",
    "- L2正则化会使参数很接近零但不为零，通过减小参数的值的大小降低复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：loss_regularization=[]\n",
    "<p>loss_regularization.append(tf.nn.l2_loss(w1))\n",
    "<p>loss_regularization.append(tf.nn.l2_loss(w2))\n",
    "<p>loss_regularization=tf.reduce_sum(loss_regularization)\n",
    "<p>loss=loss_mse+0.03*loss_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "b=lambda x:x**2\n",
    "print(b(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tensor]",
   "language": "python",
   "name": "conda-env-.conda-tensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
